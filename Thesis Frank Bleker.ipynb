{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3496f03f-bc7e-4261-95d0-d37df8556119",
   "metadata": {},
   "source": [
    "## Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e9332-aba6-4dd4-bdbc-d5ccbc3998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import IterativeImputer\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59640937-9a52-436e-aedf-26f55210df16",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4f59f-1a7d-4d80-a602-b561b201b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Survey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab565a12-4c2e-4e74-a0be-d628f5ae1245",
   "metadata": {},
   "source": [
    "## Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1d21f-9fa1-41c9-96fb-f3fbaeb59636",
   "metadata": {},
   "outputs": [],
   "source": [
    "## columns are either removed because they are redundant (test questions, meta data etc.)\n",
    "## or columns are remove because questions are deemed to similar to target variable\n",
    "\n",
    "df = df.drop(['name','essround','edition','proddate','idno', 'cntry','dweight', 'pweight','domain',\n",
    "         'stratum','prob','psu','inwds', 'ainws', 'ainwe','binwe','cinwe','vteubcmb', 'dinwe', 'finwe', 'ginwe', 'hinwe', 'iinwe',\n",
    "         'kinwe', 'vinwe', 'inwde', 'jinws', 'jinwe', 'inwtm', 'vdcond', 'vdovexre', 'vdtype', 'vdtpsvre', 'vdtpitre',\n",
    "         'vdtpscre', 'vdtpaure', 'vdtpvire', 'vdtpoire', 'vdtpntre', 'vdtpapre', 'vdtprere', 'vdtpdkre', 'vdtpnare',\n",
    "         'panpriph','govmonpb','cntbrthd', 'lnghom1', 'lnghom2', 'fbrncntc', 'mbrncntc', 'region',\"admit\",\"showcv\",\"eisced\",\"pdwrk\",\"edctn\", \n",
    "              \"uempli\",\"dsbld\",\"rtrd\",\"cmsrv\",\"hswrk\",\"dngoth\",\"dngref\",\"dngdk\",\"dngna\",\"pdwrkp\",\"edctnp\",\"uemplap\",\n",
    "              \"uemplip\",\"dsbldp\",\"dsbldp\",\"rtrdp\",\"cmsrvp\",\"hswrkp\",\"dngothp\",\"dngdkp\",\"dngnapp\",\"dngrefp\",\"dngnap\",\n",
    "              \"dscrsex\",\"dscrdsb\",\"dscroth\",\"dscrdk\",\"dscrref\",\"dscrnap\",\"dscrna\",\"uempla\",\n",
    "             \"eiscedf\",\"edulvlfb\",\"emprf14\",\"occf14b\",\"eiscedm\",\"emprm14\",\"atncrse\",\"anctry1\",\"regunit\",\"acchome\",\"accwrk\",\"accmove\",\n",
    "              \"accoth\",\"accnone\",\"accref\",\"accdk\",\"accna\",\"admc19\",\"hapljc19\",\"hapirc19\",\"hapwrc19\",\"hapfuc19\",\"hapfoc19\",\n",
    "              \"hapnoc19\",\"hapnwc19\",\"hapnpc19\",\"haprec19\",\"hapdkc19\",\"hapnac19\",\"edulvlb\", \"nacer2\", \"isco08\", \"dscrgnd\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9538c95-f69c-4a16-9c78-7aa159e3fd18",
   "metadata": {},
   "source": [
    "## Dealing with NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4686c2-096a-4e45-9460-892d11c00ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NA's are denoted by 6666,7777,8888,9999\n",
    "## or 666,777,888,999 etc. depending on the question\n",
    "## These numbers are changed to NaN\n",
    "\n",
    "for i in df.columns:\n",
    "    if((df[i].max() >= 666666)):\n",
    "        df[i] = df[i].replace(666666, np.nan)\n",
    "        df[i] = df[i].replace(777777, np.nan)\n",
    "        df[i] = df[i].replace(888888, np.nan)\n",
    "        df[i] = df[i].replace(999999, np.nan)\n",
    "    elif((df[i].max() >= 66666)):\n",
    "        df[i] = df[i].replace(66666, np.nan)\n",
    "        df[i] = df[i].replace(77777, np.nan)\n",
    "        df[i] = df[i].replace(88888, np.nan)\n",
    "        df[i] = df[i].replace(99999, np.nan)\n",
    "    elif((df[i].max() >= 6666)):\n",
    "        df[i] = df[i].replace(6666, np.nan)\n",
    "        df[i] = df[i].replace(7777, np.nan)\n",
    "        df[i] = df[i].replace(8888, np.nan)\n",
    "        df[i] = df[i].replace(9999, np.nan)\n",
    "    elif((df[i].max() >= 666)):\n",
    "        df[i] = df[i].replace(555, np.nan)\n",
    "        df[i] = df[i].replace(666, np.nan)\n",
    "        df[i] = df[i].replace(777, np.nan)\n",
    "        df[i] = df[i].replace(888, np.nan)\n",
    "        df[i] = df[i].replace(999, np.nan)\n",
    "    elif((df[i].max() >= 66)):\n",
    "        df[i] = df[i].replace(66, np.nan)\n",
    "        df[i] = df[i].replace(77, np.nan)\n",
    "        df[i] = df[i].replace(88, np.nan)\n",
    "        df[i] = df[i].replace(99, np.nan)\n",
    "    elif((df[i].max() >= 6)):\n",
    "        df[i] = df[i].replace(6, np.nan)\n",
    "        df[i] = df[i].replace(7, np.nan)\n",
    "        df[i] = df[i].replace(8, np.nan)\n",
    "        df[i] = df[i].replace(9, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723396d-448f-4143-9358-9bcebf4335eb",
   "metadata": {},
   "source": [
    "## Some Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc924f1-95b9-47a0-91ac-4fb32212744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Barplot of the distribution of the target variable\n",
    "\n",
    "def countplot(df):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    g = sns.countplot(data=df, y=\"panmonpb\")\n",
    "    plt.title(\"Label distribution for panmonpb variable\");\n",
    "    return g\n",
    "countplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3231ca5-1cd2-443a-ae84-13f5924e5a5a",
   "metadata": {},
   "source": [
    "## Splitting the target into 2 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de35006-dfe4-4e2f-b367-dab6240cce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## values 0-5 are encoded with the value 0\n",
    "## values 6-10 are encode with the value 1\n",
    "\n",
    "df[\"panmonpb\"] = df[\"panmonpb\"].replace([0, 1, 2, 3, 4, 5], 0)\n",
    "df[\"panmonpb\"] = df[\"panmonpb\"].replace([6, 7, 8, 9, 10], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab3894-d6ae-4ed2-b6a2-cadcfa5436db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a quick look at the distribution after splitting\n",
    "\n",
    "df['panmonpb'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738b5a2-6101-4d4d-849c-5948eadf69b2",
   "metadata": {},
   "source": [
    "## Dropping all columns with 20% or more NA's & dropping all rows with NA in target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d5386-fd3e-4f8a-9bfc-bd91eabb1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing all the rows that have NA in target variable\n",
    "data_cleaned = df.dropna(subset=['panmonpb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da698cb8-2dde-4189-a259-366f7019bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all columns with more than 20% missing variables are droppped\n",
    "\n",
    "cols = data_cleaned.drop([ 'panmonpb'], axis = 1).columns\n",
    "\n",
    "cols_high_nans = cols[((df[cols].isna().sum() / len(df)) > 0.2).values]\n",
    "data_cleaned = data_cleaned.drop(cols_high_nans, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72220538-8666-45a8-aba0-af6483197249",
   "metadata": {},
   "source": [
    "## EDA after NA' dropping and making the target variable binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8b3a6-af38-4a3e-9419-988f9d1ee958",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "fig = data_cleaned.groupby('gndr')['panmonpb'].plot(kind='kde')\n",
    "plt.legend([\"Male\", \"Female\"], title='Distribution')\n",
    "plt.title(\"Distribution of Genders Between the Groups\")\n",
    "plt.xticks(ticks = [0,1], labels = [\"Surveillance over Privacy\", \"Privacy over Surveillance\"])\n",
    "plt.savefig('panmonpbgenderdistr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9604516-38f7-44d5-9b80-fe6f76685af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "fig = plt.subplots(figsize = (10, 5))\n",
    "plt.pie(data_cleaned.panmonpb.value_counts(), labels = ['Privacy over Surveillance', \n",
    "                        'Surveillance over Privacy'], shadow = True,\n",
    "                         autopct='%1.0f%%')\n",
    "plt.title(\"Distribution of attitudes toward Privacy versus Surveillance\", \n",
    "          fontdict = {'fontsize':20})\n",
    "fig[0].savefig('pie_panmonpb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b64943-3583-4c64-bd29-07bf00dd0dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e949304-cc9e-44db-b9d5-3ab5174f7a78",
   "metadata": {},
   "source": [
    "## Dealing with Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2c260-11b8-44da-8534-af8073e6aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One - hot encoding\n",
    "#Getting dummies for the columns that although in the dataset are numerical, in the questionnaire\n",
    "#the answers are categorical\n",
    "\n",
    "df1_ohe = pd.get_dummies(data = data_cleaned, columns = [\"gndr\",\"vote\",\"contplt\",\"donprty\",\"badge\",\"sgnptit\",\"pbldmna\",\"bctprd\",\n",
    "                                                        \"pstplonl\",\"volunfp\",\"clsprty\",\"crmvct\",\"hlthhmp\",\"rlgblg\", \"dscrgrp\",\"ctzcntr\",\n",
    "                                                          \"brncntr\",\"feethngr\",\"facntr\",\"mocntr\",\"chpldm\",\"lvgptnea\",\"dvrcdeva\",\"maritalb\",\n",
    "                                                          \"domicil\",\"mnactic\",\"emplrel\",\"wrkctra\",\"estsz\",\"jbspv\",\"tporgwk\",\"wrkac6m\",\"uemp3m\",\n",
    "                                                          \"mbtru\",\"hincsrca\", \"livpnt\"], drop_first=True, dummy_na = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e5d45-a564-4295-afe2-796d67028e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "nan_df = df1_ohe.loc[:, df1_ohe.columns.str.endswith(\"_nan\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10872a38-8f38-463b-86ee-b79acefe5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "nan_df = df1_ohe.loc[:, df1_ohe.columns.str.endswith(\"_nan\")]\n",
    "\n",
    "#Specifying that nans in the columns after the dummy coding in order to fill them\n",
    "#in with imputation\n",
    "pattern = \"^([^_]*)_\"\n",
    "regex = re.compile(pattern)\n",
    "\n",
    "for index in df1_ohe.index:\n",
    "    for col_nan in nan_df.columns:\n",
    "        if df1_ohe.loc[index,col_nan] == 1:\n",
    "            col_id = regex.search(col_nan).group(1)\n",
    "            targets = df1_ohe.columns[df1_ohe.columns.str.startswith(col_id+'_')]\n",
    "            df1_ohe.loc[index, targets] = np.nan\n",
    "            \n",
    "df1_ohe.drop(df1_ohe.columns[df1_ohe.columns.str.endswith('_nan')], axis=1, inplace=True)\n",
    "\n",
    "data_cleaned = df1_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078a7b4-1230-4b53-95b4-c18bab6bda3c",
   "metadata": {},
   "source": [
    "## Check for the problem of multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b078a7c-cfd5-400f-bac8-26ce51977f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for the problem of multicollinearity, no values with a correlation of > 0.80\n",
    "\n",
    "cor_with_dep = data_cleaned.corr()['panmonpb']\n",
    "cols_high_cor_with_dep = cor_with_dep.index[((cor_with_dep > 0.8)).values]\n",
    "cols_high_cor_with_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b314b4-023b-4c09-9d50-5248bdbd594b",
   "metadata": {},
   "source": [
    "## Checking for the LogOdds assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e460d-413a-4598-8956-33a6eede1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for logistic regression assumptions linearity with the logodds, creating the plots\n",
    "no_dummies = pd.DataFrame()\n",
    "\n",
    "# For plotting/checking assumptions\n",
    "\n",
    "gre = sns.regplot(x= 'panfolru', y= 'panmonpb', data= df, logistic= True).set_title(\"GRE Log Odds Linear Plot\")\n",
    "gre.figure.savefig(\"gre log lin.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6472ea-0837-4cd4-8b4a-ceffe97c0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of all the features with more than 2 entries \n",
    "\n",
    "not_binary_cols = []\n",
    "for col in data_cleaned.columns:\n",
    "    if len(data_cleaned[col][data_cleaned[col].notnull()].unique()) > 2: # if feature has more than 2 non-nan entires\n",
    "        not_binary_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947a52f-2cb8-4526-8e5e-9cd70a699d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for len so we know how many plots to create\n",
    "\n",
    "len(not_binary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519e4f7-b245-4d37-a105-08542cb80efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "not_binary_cols = []\n",
    "for col in data_cleaned.columns:\n",
    "    if len(data_cleaned[col][data_cleaned[col].notnull()].unique()) > 2: # if feature has more than 2 non-nan entires\n",
    "        not_binary_cols.append(col) \n",
    "\n",
    "fsize = 8\n",
    "plt.rcParams.update({'font.size': fsize})\n",
    "sns.set_context(rc={'axes.labelsize': fsize,'axes.titlesize': fsize})\n",
    "fig1, axes1 = plt.subplots(6,6, sharey=True)\n",
    "fig2, axes2 = plt.subplots(6,6, sharey=True)\n",
    "fig3, axes3 = plt.subplots(6,6, sharey=True)\n",
    "fig4, axes4 = plt.subplots(6,6, sharey=True)\n",
    "\n",
    "i=0\n",
    "for axes in [axes1, axes2, axes3, axes4]:\n",
    "    for axy in axes:\n",
    "        for j,axx in enumerate(axy):\n",
    "            if i<len(not_binary_cols):\n",
    "                axx.tick_params(axis='x', labelsize=fsize)\n",
    "                axx.tick_params(axis='y', labelsize=fsize)\n",
    "                sns.regplot(x = not_binary_cols[i], y= 'panmonpb', data= data_cleaned, logistic= True, ax=axx)\n",
    "                i+=1\n",
    "            if j>0:         #if plot is not in first row remove ylabel\n",
    "                axx.set(ylabel=None)\n",
    "\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'qt') #to make figures pop out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb0e8a-c74a-4946-bff5-f6615b11161d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Splitting the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc0f4b-33a8-4df6-a0aa-a8a625fb912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_cleaned.drop(columns = ['panmonpb'], axis=1)\n",
    "y = data_cleaned['panmonpb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c8a08-cdab-4a71-84f4-40d0bd5cab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75d987-4749-4bb3-8d8e-03e5892375c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_cleaned = data_cleaned.drop([ 'panmonpb'], axis = 1).select_dtypes('number').columns\n",
    "cat_cols_cleaned = data_cleaned.drop([ 'panmonpb'], axis = 1).select_dtypes('object').columns\n",
    "\n",
    "num_cols_with_na = num_cols_cleaned[X_train[num_cols_cleaned].isna().mean() > 0]\n",
    "num_cols_no_na = num_cols_cleaned[~(X_train[num_cols_cleaned].isna().mean() > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722785d-67d6-49f5-88b5-80a4179cf7d8",
   "metadata": {},
   "source": [
    "## Missing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4b393-648e-4c49-a146-dc6288fc28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing na\n",
    "imputer = IterativeImputer(max_iter = 10, random_state = 0)\n",
    "\n",
    "# # fit the imputer on X_train. pass only numeric columns.\n",
    "imputer.fit(X_train[num_cols_with_na])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f64e27-1e8b-4c5e-927c-9f940d5995cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the data using the fitted imputer\n",
    "X_train_impute_num = imputer.transform(X_train[num_cols_with_na])\n",
    "X_test_impute_num = imputer.transform(X_test[num_cols_with_na])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca717c3-45be-49d5-ae72-9f18049c01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the output into DataFrame. remember to pass columns used in fit/transform\n",
    "X_train_impute_num = pd.DataFrame(X_train_impute_num, columns = num_cols_with_na)\n",
    "X_test_impute_num = pd.DataFrame(X_test_impute_num, columns = num_cols_with_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa088413-5b2e-466b-9c0f-e7fafeb207ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the numerical columns and the columns that are imputed from the dataset in order to join the imputed ones\n",
    "#and create the numerical features dataset\n",
    "cols_no_na_train = X_train.drop(columns = num_cols_with_na, axis = 1)\n",
    "cols_no_na_test = X_test.drop(columns = num_cols_with_na, axis = 1)\n",
    "X_train = X_train_impute_num.join(cols_no_na_train.reset_index(drop = True))\n",
    "X_test = X_test_impute_num.join(cols_no_na_test.reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ecf7d-d31b-4cd2-a64c-bb228432df34",
   "metadata": {},
   "source": [
    "## Rescaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7c3f-71f0-41ed-ab56-32c7952ebaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescaling the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_rescaled = scaler.fit_transform(X_train)\n",
    "X_test_rescaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a6e44-59bb-434b-b42a-2279a26f543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_rescaled, columns = X_train.columns).to_csv('X_train_rescaled.csv', index=False)\n",
    "pd.DataFrame(X_test_rescaled, columns = X_test.columns).to_csv('X_test_rescaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ed0de-c1f9-46d1-b3a1-e2ab94fb6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8a23d-d5e4-4d6d-a2cd-f64dfdb0c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rescaled = pd.read_csv('X_train_rescaled.csv' )\n",
    "X_test_rescaled = pd.read_csv('X_test_rescaled.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1067c-9410-4079-a658-cd847beeb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_csv('y_test.csv' )\n",
    "y_train = pd.read_csv('y_train.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58fad8-0e3f-4ba9-8a2c-0de1845e5df0",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c045036-cc6d-47cb-bb52-6e2b09e762eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "## from https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "rfe10 = RFE(estimator=RandomForestClassifier(), n_features_to_select=10, step = 10)\n",
    "model10 = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe10),('m',model10)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "n_scores10 = cross_val_score(pipeline, X_train_rescaled, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores10), std(n_scores10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b151c-721f-4f7a-9b20-c3c382d0be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "## from https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "rfe20 = RFE(estimator=RandomForestClassifier(), n_features_to_select=20, step = 10)\n",
    "model20 = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe20),('m',model20)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "n_scores20 = cross_val_score(pipeline, X_train_rescaled, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores20), std(n_scores20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec24f7-0687-4aac-acfb-d13820f23da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "## from https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "rfe30 = RFE(estimator=RandomForestClassifier(), n_features_to_select=30, step = 10)\n",
    "model30 = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe30),('m',model30)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "n_scores30 = cross_val_score(pipeline, X_train_rescaled, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores30), std(n_scores30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f5be3-876a-40a5-a88f-f0ee62dff333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "## from https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "rfe40 = RFE(estimator=RandomForestClassifier(), n_features_to_select=40, step = 10)\n",
    "model40 = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe40),('m',model40)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "n_scores40 = cross_val_score(pipeline, X_train_rescaled, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores40), std(n_scores40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170dc02e-e599-4295-b438-2dbb5cd972d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "## from https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "rfe50 = RFE(estimator=RandomForestClassifier(), n_features_to_select=50, step = 10)\n",
    "model50 = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe50),('m',model50)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "n_scores50 = cross_val_score(pipeline, X_train_rescaled, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores50), std(n_scores50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3be19-33c1-41bc-9b54-b15c04603eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data of lists.\n",
    "data = {'Features': [10,20,30,40,50],\n",
    "        'Accuracy': [round(mean(n_scores10),3),round(mean(n_scores20),3),round(mean(n_scores30),3),round(mean(n_scores40),3),round(mean(n_scores50),3)]}\n",
    "\n",
    "# Create DataFrame\n",
    "df23 = pd.DataFrame(data)\n",
    "\n",
    "## create plot\n",
    "ax = sns.barplot(x='Features', y='Accuracy',\n",
    "                 data=df23,\n",
    "                 errwidth=0, color = \"blue\")\n",
    " \n",
    "# now simply assign the bar values to\n",
    "# each bar by passing containers method\n",
    "# to bar_label function\n",
    "ax.bar_label(ax.containers[0])\n",
    "\n",
    "\n",
    "ax.figure.savefig(\"Features versus accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374c221-ec17-4657-8ffe-113044b87d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "pd.DataFrame(X_train_rescaled, columns = X_train.columns).to_csv('X_train_rescaled.csv', index=False)\n",
    "pd.DataFrame(X_test_rescaled, columns = X_test.columns).to_csv('X_test_rescaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2572a5-8e17-48b0-9168-8e7bf5731c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "X_train_rescaled = pd.read_csv('X_train_rescaled.csv' )\n",
    "X_test_rescaled = pd.read_csv('X_test_rescaled.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470aed87-18b1-4792-888f-df409a546139",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823800fc-57f9-4751-bffa-b638ec414991",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "y_test = pd.read_csv('y_test.csv' )\n",
    "y_train = pd.read_csv('y_train.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658a088-ef1d-4ceb-b3fa-6bc2f1f811c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "rfe40 = RFE(estimator=RandomForestClassifier(), n_features_to_select=40, step = 10)\n",
    "selector1 = rfe40.fit(X_train_rescaled, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e093448-0c7a-4f19-adf8-af5b38e67ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "X_train_rfe40 = X_train_rescaled.drop(X_train_rescaled.columns.difference(X_train_rescaled.columns[selector1.support_]), axis=1)\n",
    "X_test_40 = X_test_rescaled.drop(X_test_rescaled.columns.difference(X_test_rescaled.columns[selector1.support_]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ad842-d4d1-43e0-8037-f74cc884afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a plot of the most important features\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_rfe40, y_train.values.ravel())\n",
    "# summarize feature importance\n",
    "imp = pd.DataFrame()\n",
    "\n",
    "imp[\"var\"] = X_train_rfe40.columns\n",
    "\n",
    "importance = model.feature_importances_\n",
    "\n",
    "imp[\"imp\"] = importance\n",
    "\n",
    "imp = imp.sort_values(\"imp\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "\n",
    "# PRINT OUT THE DATA FRAME\n",
    "table = imp.sort_values(by = 'imp', ascending = False)\n",
    "\n",
    "### Select top 40 highly correlated features\n",
    "selected_features_tree =  list(table.loc[0:40]['var'])\n",
    "reduced_variables = X_train_rfe40[selected_features_tree]\n",
    "\n",
    "fig = table[0:10].plot.barh(x='var', y='imp')\n",
    "\n",
    "plt.title(\"Top 10 features selected with Random Forest algorithm\")\n",
    "plt.show()\n",
    "plt.savefig('featureimp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f3d33-eb71-41e8-8ecb-f7a89f90b403",
   "metadata": {},
   "source": [
    "## log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fefb5-7e87-4f29-90bb-e4c62d85766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "#Model without tuning\n",
    "clf = LogisticRegression(random_state = 0, class_weight = None, max_iter=500)\n",
    "model_res = clf.fit(X_train_rescaled, y_train)\n",
    "\n",
    "test_log_no_tun = model_res.predict(X_test_rescaled)\n",
    "\n",
    "print(\"The accuracy for the logistic regression model without tuning is:\", \n",
    "      balanced_accuracy_score(y_test, test_log_no_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the logistic regression model without tuning is:\", \n",
    "       confusion_matrix(y_test, test_log_no_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de0066-f187-4f6e-8a06-d0d48ebcbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNING LOGISTIC REGRESSION\n",
    "\n",
    "#https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "# Define models and parameters\n",
    "model = LogisticRegression(random_state = 0, class_weight = None, max_iter=1000)\n",
    "c_values = [100, 50, 20, 5, 1.0, 0.5, 0.1, 0.05, 0.01]\n",
    "penalty = [\"l1\", \"l2\", \"elasticnet\", None]\n",
    "\n",
    "# Define grid search\n",
    "grid = dict(C=c_values, penalty = penalty)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
    "\n",
    "#Fitting the model\n",
    "random_search = RandomizedSearchCV(estimator = model, param_distributions = grid, \n",
    "                                   cv = cv, scoring = \"accuracy\", n_iter = 20)\n",
    "random_result = random_search.fit(X_train_rescaled, y_train)\n",
    "\n",
    "#Summarizing the results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "means = random_result.cv_results_['mean_test_score']\n",
    "stds = random_result.cv_results_['std_test_score']\n",
    "params = random_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "#Selecting the parameters from the best performing model and testing it on test data\n",
    "\n",
    "final_model_lg = random_result.best_estimator_\n",
    "final_model_fitting = final_model_lg.fit(X_train_rescaled, y_train)\n",
    "\n",
    "#Predicting the data on the test set\n",
    "\n",
    "test_log_tun = final_model_fitting.predict(X_test_rescaled)\n",
    "\n",
    "\n",
    "print(\"The accuracy for the logistic regression model with tuning is:\", \n",
    "      accuracy_score(y_test, test_log_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the logistic regression model with tuning is:\", \n",
    "       confusion_matrix(y_test, test_log_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96050eb7-6098-44e5-8a15-a1a568b27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "ax = sns.heatmap(confusion_matrix(y_test, test_rf_tun)/np.sum(confusion_matrix(y_test, test_rf_tun)), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "ax.yaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "sns.set(font_scale=0.5)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c78a35-9da5-4ccb-9a3d-336ba1b5052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "#Model without tuning, 40 features\n",
    "clf = LogisticRegression(random_state = 0, class_weight = None, max_iter=500)\n",
    "model_res = clf.fit(X_train_rfe40, y_train)\n",
    "\n",
    "test_log_no_tun = model_res.predict(X_test_40)\n",
    "\n",
    "print(\"The accuracy for the logistic regression model without tuning is:\", \n",
    "      balanced_accuracy_score(y_test, test_log_no_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the logistic regression model without tuning is:\", \n",
    "       confusion_matrix(y_test, test_log_no_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf944cc-28c0-475f-b573-47eb75b28d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNING LOGISTIC REGRESSION\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "#https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "# Define models and parameters\n",
    "model = LogisticRegression(random_state = 1, class_weight = None, max_iter=1000)\n",
    "c_values = [100, 50, 20, 5, 1.0, 0.5, 0.1, 0.05, 0.01]\n",
    "penalty = [\"l1\", \"l2\", \"elasticnet\", None]\n",
    "# Define grid search\n",
    "grid = dict(C=c_values, penalty = penalty)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
    "\n",
    "#Fitting the model\n",
    "random_search = RandomizedSearchCV(estimator = model, param_distributions = grid, \n",
    "                                   cv = cv, scoring = \"accuracy\", n_iter = 20)\n",
    "random_result = random_search.fit(X_train_rfe40, y_train)\n",
    "\n",
    "#Summarizing the results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "means = random_result.cv_results_['mean_test_score']\n",
    "stds = random_result.cv_results_['std_test_score']\n",
    "params = random_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "#Selecting the parameters from the best performing model and testing it on test data\n",
    "\n",
    "final_model_lg = random_result.best_estimator_\n",
    "final_model_fitting = final_model_lg.fit(X_train_rfe40, y_train)\n",
    "\n",
    "#Predicting the data on the test set\n",
    "\n",
    "test_log_tun = final_model_fitting.predict(X_test_40)\n",
    "\n",
    "\n",
    "print(\"The accuracy for the logistic regression model with tuning is:\", \n",
    "      accuracy_score(y_test, test_log_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the logistic regression model with tuning is:\", \n",
    "       confusion_matrix(y_test, test_log_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421da143-ebc0-4b3a-b098-5cd4618c2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'qt') #to make figures pop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e4174-af4f-43c9-930c-d2ef6185105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "ax = sns.heatmap(confusion_matrix(y_test, test_log_tun)/np.sum(confusion_matrix(y_test, test_log_tun)), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "ax.yaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "sns.set(font_scale=0.50)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f458ed-3cca-4a34-8a68-cb4693132a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# The model without fine tuning\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "clf_rf= RandomForestClassifier(random_state = 0, class_weight = None)\n",
    "model_res_rf = clf_rf.fit(X_train_rescaled, y_train)\n",
    "\n",
    "test_rf_no_tun = model_res_rf.predict(X_test_rescaled)\n",
    "\n",
    "print(\"The accuracy for the Random Forests model without tuning is:\", \n",
    "      accuracy_score(y_test, test_rf_no_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the Random Forests model without tuning is:\", \n",
    "       confusion_matrix(y_test, test_rf_no_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7c93a-ee3c-4bfa-a2e5-618bcd6f3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNING RANDOM FOREST~~~~~~~~~~~~\n",
    "#https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "model_rf = RandomForestClassifier(random_state = 0, class_weight=None)\n",
    "\n",
    "# define the grid search parameters\n",
    "n_estimators= [10,50, 100, 200, 500, 750, 1000]\n",
    "max_features=[1, X_train_rescaled.shape[1]]\n",
    "param_grid_rf = dict(n_estimators=n_estimators, max_features=max_features)\n",
    "cv_rf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 3, random_state = 0)\n",
    "\n",
    "#Fitting the model\n",
    "random_search_rf = RandomizedSearchCV(model_rf, param_distributions=param_grid_rf,\n",
    "                    n_jobs = -1, cv = cv_rf, scoring = 'accuracy', n_iter = 20)\n",
    "random_result_rf = random_search_rf.fit(X_train_rescaled, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (random_result_rf.best_score_, random_result_rf.best_params_))\n",
    "means_rf = random_result_rf.cv_results_['mean_test_score']\n",
    "stds_rf = random_result_rf.cv_results_['std_test_score']\n",
    "params_rf = random_result_rf.cv_results_['params']\n",
    "for mean_rf, stdev_rf, param_rf in zip(means_rf, stds_rf, params_rf):\n",
    "    print(\"%f (%f) with: %r\" % (mean_rf, stdev_rf, param_rf))\n",
    "\n",
    "#Fitting the best model in the training data\n",
    "\n",
    "final_model_rf = random_result_rf.best_estimator_\n",
    "final_model_fitting_rf = final_model_rf.fit(X_train_rescaled, y_train)\n",
    "\n",
    "#Predicting the data on the test set\n",
    "#Selecting the parameters from the best performing model and testing it on test data\n",
    "\n",
    "test_rf_tun = final_model_fitting_rf.predict(X_test_rescaled)\n",
    "\n",
    "print(\"The accuracy for the Random Forest model with tuning is:\", \n",
    "      accuracy_score(y_test, test_rf_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the Random Forest model with tuning is:\", \n",
    "       confusion_matrix(y_test, test_rf_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df167db6-a91e-4c43-b166-f3813d014746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# The model without fine tuning, 40 features\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "clf_rf= RandomForestClassifier(random_state = 0, class_weight = None)\n",
    "model_res_rf = clf_rf.fit(X_train_rfe40, y_train)\n",
    "\n",
    "test_rf_no_tun = model_res_rf.predict(X_test_40)\n",
    "\n",
    "print(\"The accuracy for the Random Forests model without tuning is:\", \n",
    "      accuracy_score(y_test, test_rf_no_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the Random Forests model without tuning is:\", \n",
    "       confusion_matrix(y_test, test_rf_no_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7a35-6308-4064-90f6-0119d934e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNING RANDOM FOREST~~~~~~~~~~~~\n",
    "#https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "# The model with fine tuning, 40 features\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "model_rf = RandomForestClassifier(random_state = 0, class_weight=None)\n",
    "\n",
    "# define the grid search parameters\n",
    "n_estimators= [10,50, 100, 200, 500, 750, 1000]\n",
    "max_features=[1, X_train_rfe40.shape[1]]\n",
    "param_grid_rf = dict(n_estimators=n_estimators, max_features=max_features)\n",
    "cv_rf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 3, random_state = 0)\n",
    "\n",
    "#Fitting the model\n",
    "random_search_rf = RandomizedSearchCV(model_rf, param_distributions=param_grid_rf,\n",
    "                    n_jobs = -1, cv = cv_rf, scoring = 'accuracy', n_iter = 20)\n",
    "random_result_rf = random_search_rf.fit(X_train_rfe40, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (random_result_rf.best_score_, random_result_rf.best_params_))\n",
    "means_rf = random_result_rf.cv_results_['mean_test_score']\n",
    "stds_rf = random_result_rf.cv_results_['std_test_score']\n",
    "params_rf = random_result_rf.cv_results_['params']\n",
    "for mean_rf, stdev_rf, param_rf in zip(means_rf, stds_rf, params_rf):\n",
    "    print(\"%f (%f) with: %r\" % (mean_rf, stdev_rf, param_rf))\n",
    "\n",
    "#Fitting the best model in the training data\n",
    "\n",
    "final_model_rf = random_result_rf.best_estimator_\n",
    "final_model_fitting_rf = final_model_rf.fit(X_train_rfe40, y_train)\n",
    "\n",
    "#Predicting the data on the test set\n",
    "#Selecting the parameters from the best performing model and testing it on test data\n",
    "\n",
    "test_rf_tun = final_model_fitting_rf.predict(X_test_40)\n",
    "\n",
    "print(\"The accuracy for the Random Forest model with tuning is:\", \n",
    "      accuracy_score(y_test, test_rf_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the Random Forest model with tuning is:\", \n",
    "       confusion_matrix(y_test, test_rf_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94de1d-cac0-43d9-a6d0-663676e13ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "ax = sns.heatmap(confusion_matrix(y_test, test_rf_tun)/np.sum(confusion_matrix(y_test, test_rf_tun)), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "ax.yaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "sns.set(font_scale=0.5)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d5d71-b5b0-4dd8-ad1b-97e8e96a9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPPORT VECTOR MACHINE~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#The model without fine tuning \n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "clf_svm= SVC(random_state = 0, class_weight = None)\n",
    "model_res_svm = clf_svm.fit(X_train_rescaled, y_train.values.ravel())\n",
    "\n",
    "test_svm_no_tun = model_res_svm.predict(X_test_rescaled)\n",
    "\n",
    "print(\"The accuracy for the SVM model without tuning is:\", \n",
    "      balanced_accuracy_score(y_test.values.ravel(), test_svm_no_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the SVM model without tuning is:\", \n",
    "       confusion_matrix(y_test.values.ravel(), test_svm_no_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8a0bd-9dee-46be-a38f-b1c0daa1be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNING SUPPORT VECTOR MACHINE~~~~~~~~~~~~~~\n",
    "#https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "#The model without fine tuning \n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "# Define model and parameters\n",
    "model_svm = SVC(random_state = 0, class_weight = None)\n",
    "kernel = ['linear', 'poly' , 'rbf', 'sigmoid']\n",
    "C = [50, 30,40, 20,10, 5, 1.0, 0.5, 0.1, 0.05, 0.01]\n",
    "\n",
    "# Define grid search\n",
    "grid_svm = dict(C = C, kernel=kernel)\n",
    "cv_svm = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 3, random_state = 0)\n",
    "#Fitting the model\n",
    "random_search_svm = RandomizedSearchCV(estimator = model_svm, \n",
    "        param_distributions = grid_svm, n_jobs = -1, cv = cv_svm, scoring = 'accuracy',\n",
    "        error_score = 0, n_iter = 40)\n",
    "random_result_svm = random_search_svm.fit(X_train_rescaled, y_train.values.ravel())\n",
    "\n",
    "# Summarize results\n",
    "\n",
    "print(\"Best: %f using %s\" % (random_result_svm.best_score_, random_result_svm.best_params_))\n",
    "means_svm = random_result_svm.cv_results_['mean_test_score']\n",
    "stds_svm = random_result_svm.cv_results_['std_test_score']\n",
    "params_svm = random_result_svm.cv_results_['params']\n",
    "for mean_svm, stdev_svm, param_svm in zip(means_svm, stds_svm, params_svm):\n",
    "    print(\"%f (%f) with: %r\" % (mean_svm, stdev_svm, param_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ab82c-8073-44c9-8927-6cd043ad66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPPORT VECTOR MACHINE~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#The model without fine tuning, 40 features\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "clf_svm= SVC(random_state = 0, class_weight = None)\n",
    "model_res_svm = clf_svm.fit(X_train_rfe40, y_train.values.ravel())\n",
    "\n",
    "test_svm_no_tun = model_res_svm.predict(X_test_40)\n",
    "\n",
    "print(\"The accuracy for the SVM model without tuning is:\", \n",
    "      accuracy_score(y_test.values.ravel(), test_svm_no_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the SVM model without tuning is:\", \n",
    "       confusion_matrix(y_test.values.ravel(), test_svm_no_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc2147-ebfc-494e-b538-53be2e7f00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNING SUPPORT VECTOR MACHINE~~~~~~~~~~~~~~\n",
    "#https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "## The model with fine tuning, 40 features\n",
    "## adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "# Define model and parameters\n",
    "model_svm = SVC(random_state = 0, class_weight = None)\n",
    "kernel = ['linear', 'poly' , 'rbf', 'sigmoid']\n",
    "C = [50, 30,40, 20,10, 5, 1.0, 0.5, 0.1, 0.05, 0.01]\n",
    "\n",
    "# Define grid search\n",
    "grid_svm = dict(C = C, kernel=kernel)\n",
    "cv_svm = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 3, random_state = 0)\n",
    "#Fitting the model\n",
    "random_search_svm = RandomizedSearchCV(estimator = model_svm, \n",
    "        param_distributions = grid_svm, n_jobs = -1, cv = cv_svm, scoring = 'accuracy',\n",
    "        error_score = 0, n_iter = 40)\n",
    "random_result_svm = random_search_svm.fit(X_train_rfe40, y_train.values.ravel())\n",
    "\n",
    "# Summarize results\n",
    "\n",
    "print(\"Best: %f using %s\" % (random_result_svm.best_score_, random_result_svm.best_params_))\n",
    "means_svm = random_result_svm.cv_results_['mean_test_score']\n",
    "stds_svm = random_result_svm.cv_results_['std_test_score']\n",
    "params_svm = random_result_svm.cv_results_['params']\n",
    "for mean_svm, stdev_svm, param_svm in zip(means_svm, stds_svm, params_svm):\n",
    "    print(\"%f (%f) with: %r\" % (mean_svm, stdev_svm, param_svm))\n",
    "    \n",
    "#Fitting the best model in the training data\n",
    "\n",
    "final_model_svm = random_result_svm.best_estimator_\n",
    "final_model_fitting_svm = final_model_svm.fit(X_train_rfe40, y_train)\n",
    "\n",
    "#Predicting the data on the test set\n",
    "#Selecting the parameters from the best performing model and testing it on test data\n",
    "\n",
    "test_svm_tun = final_model_fitting_svm.predict(X_test_40)\n",
    "\n",
    "print(\"The accuracy for the Random Forest model with tuning is:\", \n",
    "      accuracy_score(y_test, test_svm_tun))\n",
    "      \n",
    "print(\"The confusion matrix for the Random Forest model with tuning is:\", \n",
    "       confusion_matrix(y_test, test_svm_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27cffd-66c9-45bf-b7d5-63b8d9081567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### adapted from https://github.com/kantarafr/Thesis/blob/main/Pre-processing.py\n",
    "\n",
    "ax = sns.heatmap(confusion_matrix(y_test, test_svm_tun)/np.sum(confusion_matrix(y_test, test_svm_tun)), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "ax.yaxis.set_ticklabels(['Surveillance over Privacy','Privacy over Surveillance'])\n",
    "sns.set(font_scale=0.5)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
